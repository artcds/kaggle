---
title: "Titanic Competition - Automating model selection using Caret"
author: "Cristina Santana Souza"
date: "March 2019"
output:
  html_document:
    number_sections: true # Sets an automatic number for each item
    code_folding: show    # Shows the source code by default
    toc: true             # Defines a summary for the document
    fig_width: 10         # Standard figure width
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal

Predict which people will survive and create an automated process to test multiple models simultaneously and choose the one that got the best performance.

# Fields

* PassengerId: Id given to each traveler on the boat
* Survived: 0 = No, 1 = Yes
* Pclass: Passenger class, it has three possible values: 1,2,3 (first, second and third class)
* Name: Name of the passenger
* Sex: Sex of the passenger
* Age: Age of the passenger
* SibSp: Number of siblings and spouses traveling with the passenger
* Parch: Number of parents and children traveling with the passenger
* Ticket: Ticket number
* Fare: Ticket fare
* Cabin: Cabin number
* Embarked: The embarkation, three possible values: S,C,Q

```{r ini}
# Informs the process start date/time
dateIni <- Sys.time()
cat("\n Start of execution: ", as.character(dateIni))
```

```{r libraries, include=FALSE}

# Lists the libraries that will be used
libs = c("data.table","ggplot2","randomForest","ROSE","DMwR","corrplot","caret",
         "xgboost","e1071","PRROC","klaR","dplyr","tidyr")

# Loads or installs the package
for(i in libs)
{
  if(i %in% row.names(installed.packages()) == FALSE){
    cat("Load/install the library: ", i, "\n\n")
    install.packages(i, repos = "http://cran.us.r-project.org")
    library(i, character.only = TRUE)
  } else {
    cat("Load the library: ", i, "\n\n")
    library(i, character.only = TRUE)
  }
}

```

# Load datasets

```{r load}
trainPath <- "../input/train.csv"
testPath  <- "../input/test.csv"

trainOri <- read.csv(trainPath, na.strings = c("NA","NaN", ""))
testOri <- read.csv(testPath, na.strings = c("NA","NaN", ""))
```

# Function "createModels"

The "createModels" function was created to facilitate the training of the model, it generates five models according to the algorithm and the parameters informed, analyzes the most important variables, generates a ROC chart comparing the models and informs which one obtained the best performance.

**Models:**
  
 * Model 1 - Original - No resampling technique
 * Model 2 - Oversampling: this technique includes elements in the minority class
 * Model 3 - Undersampling: this technique excludes elements in the majority class
 * Model 4 - Random Oversampling Examples: also called ROSE, it generates artificial balanced samples
 * Model 5 - Synthetic Minority Oversampling Technique: also called SMOTE, it creates artificial examples by interpolation

**Parameters:**

 * formula: formula with the values of X and Y that will be used to generate the model
 * data: dataframe with the data that will be trained                                    
 * methods: name of the algorithms that will be used, example "knn","rf"
 * ctrl: informs the control parameters                                            
 * preProcess: it specifies what pre-processing should occur                                                    
 * tuneLength: it defines the total number of parameter combinations that will be evaluated                     

```{r createModels, include=FALSE}
createModels <- function(formula, 
                         data, 
                         methods, 
                         ctrl, 
                         preProcess = NULL,
                         tuneLength = 1) {
  
  # Suppresses the warning messages
  options(warn=-1)
  
  # Sets metric and seed
  metric <- "ROC"
  seed   <- 54321
  
  # Transforms the variable into type formula
  formula <- as.formula(formula)
  
  # Creates lists to store models
  modelsOri    <- list()
  modelsUp     <- list()  
  modelsDown   <- list()
  modelsRose   <- list()
  modelsSmote  <- list()
  
  # Creates a dataframe to store the performances
  score <- data.frame(method=character(),
                      score=numeric(),
                      model=character(), 
                      stringsAsFactors=FALSE)
  
  # Generates 5 models (original, up, down, ROSE and SMOTE) for each informed method
  for (i in methods) {
    
    tryCatch({
      
      cat("METHOD: ", i, '\n\n')
      
      #################################################################
      #         MODEL 1 - Original - No resampling technique          #
      #################################################################
      ctrl$sampling <- NULL
      nameOri <- paste0(i,".ori")
      set.seed(seed)
      modelsOri[[nameOri]] <- train(form = formula, 
                                    data = data, 
                                    method = i, 
                                    metric = metric,
                                    preProcess = preProcess,
                                    tuneLength = tuneLength,
                                    trControl = ctrl)

      cat('Importance of variables \n\n')
      tryCatch({
        importanceOri <- varImp(modelsOri[[nameOri]], scale=FALSE)
        print(importanceOri)

        # Displays the plot with the importance of the variables
        print(plot(importanceOri))

      }, error=function(e){
        cat("It wasn't possible to verify the importance of the variables: ",i, " - ERROR :",conditionMessage(e),"\n")})

      # Print the score
      cat("\n\n PERFORMANCE - ORIGINAL MODEL: \n\n")
      print(getTrainPerf(modelsOri[[nameOri]]))
      
      # Save the model
      nameFile <- paste0(nameOri,".model.rds")
      saveRDS(modelsOri[[nameOri]],paste0("./",nameFile))
      
      # Store the score in the dataframe
      score[nrow(score) + 1,] = list(nameOri,
                                     getTrainPerf(modelsOri[[nameOri]])[, "TrainROC"],
                                     nameFile)
      
      #################################################################
      #              MODEL 2 - UP (Oversampling)                      #
      #################################################################
      ctrl$sampling <- "up"
      
      nameUp <- paste0(i,".up")
      set.seed(seed)
      modelsUp[[nameUp]] <- train(form = formula, 
                                  data = data, 
                                  method = i, 
                                  metric = metric,
                                  preProcess = preProcess,
                                  tuneLength = tuneLength,
                                  trControl = ctrl)
      
      # Print the score
      cat("\n\n PERFORMANCE - UP MODEL: \n\n")
      print(getTrainPerf(modelsUp[[nameUp]]))
      
      # Save the model
      nameFile <- paste0(nameUp,".model.rds")
      saveRDS(modelsUp[[nameUp]],paste0("./",nameFile))
      
      # Store the score in the dataframe
      score[nrow(score) + 1,] = list(nameUp,
                                     getTrainPerf(modelsUp[[nameUp]])[, "TrainROC"],
                                     nameFile)
      
      #################################################################
      #                 MODEL 3 - DOWN (Undersampling)                #
      #################################################################
      ctrl$sampling <- "down"
      
      nameDown <- paste0(i,".down")
      set.seed(seed)
      modelsDown[[nameDown]] <- train(form = formula, 
                                      data = data, 
                                      method = i,
                                      metric = metric,
                                      preProcess = preProcess,
                                      tuneLength = tuneLength,
                                      trControl = ctrl)
      
      # Print the score
      cat("\n\n PERFORMANCE - DOWN MODEL: \n\n")
      print(getTrainPerf(modelsDown[[nameDown]]))
      
      # Save the model
      nameFile <- paste0(nameDown,".model.rds")
      saveRDS(modelsDown[[nameDown]],paste0("./",nameFile))
      
      # Store the score in the dataframe
      score[nrow(score) + 1,] = list(nameDown,
                                     getTrainPerf(modelsDown[[nameDown]])[, "TrainROC"],
                                     nameFile)
      
      
      ##########################################################################
      #              MODEL 4 - ROSE (Random Over-Sampling Examples)            #
      ##########################################################################
      ctrl$sampling <- "rose"
      
      nameRose <- paste0(i,".rose")
      set.seed(seed)
      modelsRose[[nameRose]] <- train(form = formula, 
                                      data = data, 
                                      method = i, 
                                      metric = metric,
                                      preProcess = preProcess,
                                      tuneLength = tuneLength,
                                      trControl = ctrl)
      
      # Print the score
      cat("\n\n PERFORMANCE - ROSE MODEL: \n\n")
      print(getTrainPerf(modelsRose[[nameRose]]))
      
      # Save the model
      nameFile <- paste0(nameRose,".model.rds")
      saveRDS(modelsRose[[nameRose]],paste0("./",nameFile))
      
      # Store the score in the dataframe
      score[nrow(score) + 1,] = list(nameRose,
                                     getTrainPerf(modelsRose[[nameRose]])[, "TrainROC"],
                                     nameFile)
      
      #######################################################################################
      #             MODEL 5 - SMOTE (Synthetic Minority Oversampling Technique)             #
      #######################################################################################
      ctrl$sampling <- "smote"
      
      nameSmote <- paste0(i,".smote")
      set.seed(seed)
      modelsSmote[[nameSmote]] <- train(form = formula, 
                                        data = data, 
                                        method = i, 
                                        metric = metric,
                                        preProcess = preProcess,
                                        tuneLength = tuneLength,
                                        trControl = ctrl)
      
      # Print the score
      cat("\n\n PERFORMANCE - SMOTE MODEL: \n\n")
      print(getTrainPerf(modelsSmote[[nameSmote]]))
      
      # Save the model
      nameFile <- paste0(nameSmote,".model.rds")
      saveRDS(modelsSmote[[nameSmote]],paste0("./",nameFile))
      
      # Store the score in the dataframe
      score[nrow(score) + 1,] = list(nameSmote,
                                     getTrainPerf(modelsSmote[[nameSmote]])[, "TrainROC"],
                                     nameFile)
      
      ###################################################################
      #  Evaluates the result of the original model and the resamplings #
      ###################################################################
      models <- list(original = modelsOri[[nameOri]],
                     down = modelsDown[[nameDown]],
                     up = modelsUp[[nameUp]],
                     smote = modelsSmote[[nameSmote]],
                     rose = modelsRose[[nameRose]])
      
      #Remove null values, if exists
      models[sapply(models, is.null)] <- NULL
      
      cat("EVALUATE THE MODELS USING THE ROC METRIC \n\n")
      resampling <- resamples(models)
      print(summary(resampling, metric = metric))
      
      cat("DOTPLOT \n\n")
      scales <- list(x=list(relation="free"), y=list(relation="free"))
      print(dotplot(resampling, scales=scales, main=paste("Evaluating all models of the method",i)))
      
    }, error=function(e){
      cat("It wasn't possible to train the model ", i, " - ERROR :",conditionMessage(e), "\n")          
      
    })
    
  }
  
  ################################################################
  #                   Evaluate all models                        #
  ################################################################
  
  # Concatenates all generated models
  modelsList <- c(modelsOri,modelsDown,modelsUp,modelsSmote,modelsRose)
  
  # Only if you have more than one method does the overall evaluation
  if(length(modelsOri) > 1){
    
    cat("\n\n EVALUATING THE RESULT OF ALL METHODS AND MODELS \n\n")
    
    resampling <- resamples(modelsList)
    print(summary(resampling, metric = metric))
    
    scales <- list(x=list(relation="free"), y=list(relation="free"))
    print(dotplot(resampling, scales=scales, main="Evaluating all methods used"))
  }
  
  cat("\n\n MODEL WITH THE BEST PERFORMANCE: \n\n")
  best <- score %>% top_n(1, score) %>% head(1)
  print(best)
  
  # Delete all models except winner
  rdsFiles = list.files(pattern='.rds')
  rdsFiles <- rdsFiles [! rdsFiles %in% best$model]
  file.remove(rdsFiles)

  # Returns the name of the best model
  return(best$model)
  
}

```

# Exploratory Analysis and Data Transformation

* Checks the type of columns and the first 5 lines:

Analyzing the column "Name" we can see in the name the title of the person, e.g. in the name "Kelly, Mr. James" the title is "Mr". A new variable will be created for title analysis.

```{r strTrain}
## Train
glimpse(trainOri)
sapply(trainOri, class)
head(trainOri,5)

## Test
glimpse(testOri)
sapply(testOri, class)
head(testOri,5)
```

* Check the statistics

We can verify that there are NA values in the columns "Age", "Fare", "Cabin" and "Embarked":

* Train: Age, Cabin, Embarked  
* Test: Age, Cabin, Fare

```{r summary}
summary(trainOri)
summary(testOri)
```

* Check the percentage of NAs

The percentage of NAs in the "Cabin" column is greater than 70%, so I believe this column should be deleted.

```{r checkNA}
# Train
sapply(trainOri, function(x) round(sum(is.na(x))/nrow(trainOri) * 100,1))

# Test
sapply(testOri, function(x) round(sum(is.na(x))/nrow(testOri) * 100,1))

```

* Checks if the records are balanced

We can see that the classes are unbalanced, the number of people who did not survive the accident is greater than 61%. We will try resampling techniques to see if the model's performance will improve.

```{r balanc}
cbind(freq=table(trainOri$Survived), percent=round(prop.table(table(trainOri$Survived))*100,1))

```

* Checks the unique records for each attribute

```{r unique}
# Train
apply(trainOri,2,function(x) length(unique(x)))

# Test
apply(trainOri,2,function(x) length(unique(x)))
```

# Combine data

The datasets will be joined to facilitate the processing of data.

```{r fullData}
testOri$Survived <- NA;
fullData <- rbind(trainOri, testOri)
trainIdx <- seq(nrow(trainOri)) #Training data index
```

# Data visualizations

* Number of survivors by sex

We can observe that the number of women who survived is greater than the number of men.

```{r plotSexSurvived}
ggplot(fullData[trainIdx,], aes(Sex, fill = factor(Survived))) + 
  geom_bar(stat = "count", position = 'dodge')+
  xlab("Sex") +
  ylab("Count") +
  scale_fill_discrete(name = "Survived") + 
  ggtitle("Sex X Survived")
```

* Number of survivors per class

We can observe that the number of survivors is greater when the passenger is in the first class.

```{r plotPclassSurvived}
ggplot(fullData[trainIdx,], aes(Pclass, fill = factor(Survived))) + 
  geom_bar(stat = "count")+
  xlab("Pclass") +
  ylab("Count") +
  scale_fill_discrete(name = "Survived") + 
  ggtitle("Pclass X Survived")
```

* Generates a histogram of each attribute

```{r hist}
numberCols <- dplyr::select_if(trainOri, is.numeric)

par(mfrow=c(2,2))
for(i in 1:7) {
  hist(numberCols[,i], main=names(numberCols)[i], xlab = "")
}
```

* Generates a boxplot for each attribute

We can observe some outliers, however no treatment will be performed.

```{r boxplot}
par(mfrow=c(2,2))
for(i in 1:7) {
  boxplot(numberCols[,i], main=names(numberCols)[i])
}
```

* Generates a barplot of each attribute per class

```{r barplot}
par(mfrow=c(2,2))
for(i in 1:7) {
  barplot(table(numberCols$Survived,numberCols[,i]), 
          main=names(numberCols)[i], 
          legend.text=unique(numberCols$Survived))
}

```

# Filling NA values

The "Age" and "Fare" columns will be filled with the average per class and the column "Embarked" will be filled with the highest occurrence of values.

```{r removeNAs}
#Age and Fare
fullTemp <- fullData %>% 
            group_by(Pclass) %>%
            mutate(Age = ifelse(is.na(Age), round(mean(Age, na.rm = TRUE)), Age)) %>%
            mutate(Fare = ifelse(is.na(Fare), round(mean(Fare, na.rm = TRUE)), Fare))
fullData$Age <- fullTemp$Age
fullData$Fare <- fullTemp$Fare

#Embarked 
maxEmbarked <- names(sort(table(fullData$Embarked),decreasing = T)[1])
fullData$Embarked[is.na(fullData$Embarked)] <- maxEmbarked

```

# One-Hot-Encoding

Categorical variables will be transformed, one of the most common ways to make this transformation is to one-hot encode the categorical features, especially when there does not exist a natural ordering between the categories (e.g. a feature ‘Country’ with names such as ‘Brazil’, ‘France’, ‘Japan’, etc.).

```{r ohe}
# Sex
dummies <- predict(dummyVars(~ Sex, data = fullData), newdata = fullData)
fullData <- cbind(fullData,dummies)

# Embarked
dummies <- predict(dummyVars(~ Embarked, data = fullData), newdata = fullData)
fullData <- cbind(fullData,dummies)

# Pclass
fullData$Pclass <- factor(fullData$Pclass)
dummies <- predict(dummyVars(~ Pclass, data = fullData), newdata = fullData)
fullData <- cbind(fullData,dummies)
```

# New variables

Two new categorical variables will be created. One to store the title of the person and another to know the type of the family.

```{r newVars}
# Title
fullData$Title <- gsub('(.*, )|(\\..*)', '', fullData$Name)

## Create only one category for similar titles
officer <- c('Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev')
royalty <- c('Dona', 'Lady', 'the Countess','Sir', 'Jonkheer')
fullData$Title[fullData$Title == 'Mlle'] <- 'Miss' 
fullData$Title[fullData$Title == 'Ms']   <- 'Miss' 
fullData$Title[fullData$Title == 'Mme']  <- 'Mrs' 
fullData$Title[fullData$Title %in% royalty]  <- 'Royalty'
fullData$Title[fullData$Title %in% officer]  <- 'Officer'

## One-hot-enconding
fullData$Title <- factor(fullData$Title)
dummies <- predict(dummyVars(~ Title, data = fullData), newdata = fullData)
fullData <- cbind(fullData,dummies)

#FamilyType
fullData$FamilySize <- fullData$SibSp + fullData$Parch + 1
fullData$FamilyType[fullData$FamilySize == 1] <- 'A' #Alone
fullData$FamilyType[fullData$FamilySize > 1 & fullData$FamilySize < 5] <- 'S' #Small
fullData$FamilyType[fullData$FamilySize >= 5] <- 'B' #Big

## One-hot-enconding
fullData$FamilyType <- factor(fullData$FamilyType)
dummies <- predict(dummyVars(~ FamilyType, data = fullData), newdata = fullData)
fullData <- cbind(fullData,dummies)
```

* Generating plots with new variables

The new variables are related to the target variable.

```{r plotTitleSurvived}
ggplot(fullData[trainIdx,], aes(Title,fill = factor(Survived))) +
  geom_bar(stat = "count")+
  xlab('Title') +
  ylab("Count") +
  scale_fill_discrete(name = " Survived") + 
  ggtitle("Title X Survived")
```

```{r plotFamilyTypeSurvived}
ggplot(fullData[trainIdx,], aes(FamilyType,fill = factor(Survived))) +
  geom_bar(stat = "count")+
  xlab('FamilyType') +
  ylab("Count") +
  scale_fill_discrete(name = " Survived") + 
  ggtitle("FamilyType X Survived")
```

# Remove columns 

Unused columns will be deleted.

```{r removeColumns}
fullData$Ticket <- NULL
fullData$Cabin <- NULL
fullData$Sex <- NULL
fullData$Embarked <- NULL
fullData$Pclass <- NULL
fullData$Title <- NULL
fullData$Name <- NULL
fullData$FamilyType <- NULL
fullData$FamilySize <- NULL

glimpse(fullData)
```

# Correlation

* Checks the correlation between the variables

```{r correlacao, fig.width = 17, fig.height = 15, fig.align = "center"}
cor(fullData[trainIdx,]) %>% corrplot(addCoef.col = "grey", number.cex = 1.4)
```

# Split data into training and validation

It was used 70% for training and 30% for validation.

```{r split}
# Selecting the training variables, the "PassengerId" will not be used because it is just a ID
trainData <- subset(fullData[trainIdx, ], select=-PassengerId) 

# Changing the target variable to factor
trainData$Survived <- factor(trainData$Survived)
levels(trainData$Survived) <- c("no", "yes")
print(table(trainData$Survived, useNA = "always"))

# Divide data into training and validation
index <- createDataPartition(y = trainData$Survived, p = 0.7, list = FALSE)
train <- trainData[index,]
valid <- trainData[-index,]
```

# Training the model

This is the main purpose of the script, running multiple models and choosing the one that got the best performance in training to predict test data.

Only a few algorithms were executed, but this script can be adapted to use several others, Caret currently has 238 algorithms. You can run this code "names(getModelInfo())" or access the link below for more details.

See: http://topepo.github.io/caret/available-models.html

```{r train}
# Choose the models that will be trained
methods <- list("knn","nb","glmboost")

# Set the variables for the function "createModels"
formula <- "Survived ~ ."
preProcess <- c("center", "scale")          
tuneLength <- 25                             
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 3, 
                     allowParallel = TRUE,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Train the models
nameModel <- createModels(formula,train,methods,ctrl,preProcess)

cat("Best model: ",nameModel)
```

# Running the model in the validation data

The model that obtained the best performance was used in the validation data and to predict the test data.

```{r valid }
# Recover saved model
bestModel <- readRDS(paste0("./",nameModel))

# Check the best of the hyperparameters
print(bestModel$bestTune)

# Testing the model in the validation data
xValid <- subset(valid, select=-Survived)
yValid <- valid$Survived

predValues <- predict(object = bestModel, 
                       newdata = xValid, 
                       type = "raw")
head(predValues,5)

# Generates confusion matrix with positive class (yes)
confusionMatrix(predValues,yValid,positive = "yes")

# Returns the probability of the positive class (yes)
predProbs <- predict(object = bestModel, newdata = xValid, type="prob")[,2] 
head(predProbs)

predPos <- predProbs[yValid=="yes"]  #prediction for true positives
predNeg <- predProbs[yValid=="no"]   #prediction for true negatives

# Generates a plot showing the ROC curve and PR

# ROC Curve    
roc <- PRROC::roc.curve(scores.class0 = predPos, 
                        scores.class1 = predNeg, 
                        curve = T)
print(roc)
plot(roc)

# PR Curve
pr <- PRROC::pr.curve(scores.class0 = predPos, 
                      scores.class1 = predNeg, 
                      curve = T)
print(pr)
plot(pr)
```

# Generating predictions for test data

```{r test }
# Selects the columns of the dataframe that will be used
testData <- subset(fullData[-trainIdx, ], select=-Survived) 
print(head(testData,5))
print(str(testData))

# Testing the model in test data
predTestValues <- predict(object = bestModel, 
                          newdata = testData[,-1], #The PassengerId will not be used
                          type = "raw")
print(head(predTestValues,5))

# Converts data to a dataframe
predTest <- as.data.frame(predTestValues)
print(head(predTest,10))
```

# Submitting data for Kaggle validation

```{r submit}
# Creates the object that will store the data that will be submitted
sub <- data.table(PassengerId = testData$PassengerId, Survived = NA)
sub$Survived = as.numeric(ifelse(predTest == "no", 0, 1))
print(head(sub,10))

# Save the CSV file
nameFile <- paste0(nameModel,".submission.csv")
fwrite(sub, nameFile)

# Informs the end date/time of the process
dateFin <- Sys.time()
cat("\n End of execution: ", as.character(dateFin))
```
